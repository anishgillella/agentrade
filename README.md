# AI Trading Intelligence System â€” MVP Architecture

An autonomous, multi-agent trading system powered by LLMs that combines real-time news sentiment analysis, quantitative signal reasoning, and investor personality archetypes to generate trading signals across multiple asset classes.

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Technology Stack](#technology-stack)
4. [Core Components & LLM Reasoning Loops](#core-components--llm-reasoning-loops)
5. [Data Flow & Kafka Topics](#data-flow--kafka-topics)
6. [Personality Modes (LLM-based)](#personality-modes-llm-based)
7. [MVP Scope & Constraints](#mvp-scope--constraints)
8. [Frontend Architecture](#frontend-architecture)
9. [Project Structure](#project-structure)
10. [Getting Started](#getting-started)
11. [Future Phases](#future-phases)

---

## ğŸ¯ Project Overview

This system implements a **three-agent LLM architecture** using LangGraph that processes real-time news, reasons about market signals, applies investor-specific decision logic, and generates trading recommendations for live execution.

**Key Innovation:** Instead of hard-coded rules, each agent uses an LLM to reason through multi-step decision logic:
- News Agent reasons about deduplication, sentiment, and impact
- Investment Agent reasons about signal fusion and correlation
- Personality Agent reasons through an investor's decision-making lens
- Research Agent (optional) conducts deep reasoning on large positions

### Core Problem Solved
- Most trading systems use rigid rules and miss nuance.
- This system leverages LLM reasoning to understand context, detect contradictions, and make probabilistic decisions.
- Personality modes allow the same signals to be interpreted through different investor reasoning styles (Buffett = value; Soros = momentum; Cathie = innovation).

### MVP Goals
- Validate that LLM-based signal reasoning produces edge over rule-based systems.
- Demonstrate multi-agent LangGraph architecture with Kafka streaming.
- Support mode-switching without disrupting existing positions.
- Backtest concept before scaling.

---

## ğŸ—ï¸ System Architecture

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEWS SOURCES (Live APIs)                        â”‚
â”‚  - Reddit (PRAW)                                 â”‚
â”‚  - Twitter/X (tweepy)                            â”‚
â”‚  - CoinTelegraph (RSS/web scrape)                â”‚
â”‚  - NewsAPI (free tier)                           â”‚
â”‚  Expected: ~10 articles/minute                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      [Kafka Producer: raw_news]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEWS AGENT (LangGraph State Graph)              â”‚
â”‚  â”œâ”€ Deduplication Node                          â”‚
â”‚  â”‚  (LLM: "Are these the same event?")          â”‚
â”‚  â”œâ”€ Event Extraction Node                       â”‚
â”‚  â”‚  (LLM: "What's happening? Extract ticker")   â”‚
â”‚  â”œâ”€ Sentiment Analysis Node                     â”‚
â”‚  â”‚  (LLM: "Analyze sentiment + impact level")   â”‚
â”‚  â””â”€ Signal Emission Node                        â”‚
â”‚     (Emit structured signal to Kafka)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      [Kafka: processed_signals]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INVESTMENT AGENT (LangGraph State Graph)        â”‚
â”‚  â”œâ”€ Signal Collection Node                      â”‚
â”‚  â”‚  (Consume news signals + fetch quant data)   â”‚
â”‚  â”œâ”€ Signal Fusion Reasoning Node                â”‚
â”‚  â”‚  (LLM: "How do sentiment, RSI, volume        â”‚
â”‚  â”‚   align? What's the overall picture?")       â”‚
â”‚  â”œâ”€ Position Sizing Node                        â”‚
â”‚  â”‚  (LLM: "Given risks, what size?")            â”‚
â”‚  â””â”€ Trade Recommendation Node                   â”‚
â”‚     (LLM: "BUY/SELL/HOLD + confidence")         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      [Kafka: trade_signals (pre-execution)]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PERSONALITY AGENT (LangGraph State Graph)       â”‚
â”‚  â”œâ”€ Load Persona Context Node                   â”‚
â”‚  â”‚  (Redis: fetch current mode context)         â”‚
â”‚  â”œâ”€ Persona Reasoning Node                      â”‚
â”‚  â”‚  (LLM: "How would [persona] see this trade?")â”‚
â”‚  â”œâ”€ Decision Adjustment Node                    â”‚
â”‚  â”‚  (LLM: "Adjust confidence or veto?")         â”‚
â”‚  â””â”€ Validation Node                             â”‚
â”‚     (Emit final trade decision)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      [Kafka: validated_trades]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OPTIONAL: PARALLEL RESEARCH AGENT               â”‚
â”‚  â”œâ”€ Large Trade Filter Node                     â”‚
â”‚  â”‚  (Size > $10k threshold)                     â”‚
â”‚  â”œâ”€ Research Reasoning Node                     â”‚
â”‚  â”‚  (LLM: "Research this company")              â”‚
â”‚  â”œâ”€ Call Parallel API Tool                      â”‚
â”‚  â”‚  (Fetch deep research findings)              â”‚
â”‚  â””â”€ Signal Validation Node                      â”‚
â”‚     (LLM: "Do findings confirm or deny?")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      [Kafka: researched_trades]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BROKER AGENT (Hybrid: Rules + Light LLM)             â”‚
â”‚  â”œâ”€ Risk Check Node (Rules)                     â”‚
â”‚  â”‚  (Position limits, leverage checks)          â”‚
â”‚  â”œâ”€ Order Execution Node (Rules)                â”‚
â”‚  â”‚  (Place order via Binance API)               â”‚
â”‚  â””â”€ Trade Tracking Node                         â”‚
â”‚     (Log to SQLite, emit to Kafka)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      [Kafka: executed_trades]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYTICS & STATE STORE                         â”‚
â”‚  â”œâ”€ SQLite: trades, signals, positions          â”‚
â”‚  â”œâ”€ Redis: mode persistence, LLM context cache  â”‚
â”‚  â”œâ”€ Streamlit Dashboard                         â”‚
â”‚  â””â”€ Performance metrics (Sharpe, Win Rate, etc.)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

| Layer | Tool/Framework | Purpose | Why |
|-------|---|---|---|
| **Language** | TypeScript + Node.js | Application runtime | Native type safety, async/await support |
| **Streaming** | Apache Kafka (Docker) | Real-time event bus | Decoupled agents, replay-able history, scalable |
| **LLM Agent Framework** | LangGraph | Multi-agent orchestration | First-class agent loops, tool use, state management |
| **LLM Provider** | Claude (Anthropic) or OpenAI | LLM reasoning | Fast, reliable reasoning over text + data |
| **News Ingestion** | PRAW, tweepy, requests | Reddit, Twitter, web scraping | Free, well-maintained, real-time |
| **Tools for Agents** | @langchain/core Tools | Structured tool use | Sentiment scoring, quant data fetching, API calls |
| **Vector DB** | LanceDB (local) | News deduplication | Fast semantic similarity, no cloud deps |
| **Embeddings** | sentence-transformers (local) | Sentence embeddings | Lightweight, runs locally for dedup |
| **State Store** | Redis + SQLite | Mode persistence, trade tracking | Fast KV for mode + context, persistent audit trail |
| **Database** | SQLite (local) | Trades, signals, metrics | Zero-ops, ACID compliance, easy to inspect |
| **Dashboard** | Streamlit | Live monitoring & visualization | Fast prototyping, real-time updates |
| **Data Serialization** | Zod (TypeScript) | Message validation | Type-safe schemas, better than Avro for MVP |
| **Logging** | Pino | Structured logging | Fast JSON logging, trace agent reasoning |
| **Web Research** | Parallel API | Deep research on positions | High-accuracy fact gathering for large trades |

---

## ğŸ§  Core Components & LLM Reasoning Loops

### 1. News Agent (LangGraph State Graph)

**Purpose:** Consume raw news from multiple sources, reason about deduplication, analyze sentiment, extract impact, emit structured signals.

**LLM Reasoning Loops:**

**Node 1: Deduplication Reasoning**
- Input: Raw articles (headlines, URLs, sources)
- LLM Task: "Given these articles, determine if they describe the same event or different events. Consider: similar headlines, same sources, correlated timing, overlapping subject matter."
- Output: `{ is_duplicate: boolean, event_id: UUID, canonical_url: string }`
- Early Exit: If duplicate, stop processing (return null to Kafka)

**Node 2: Event Extraction & Contextualization**
- Input: Non-duplicate articles
- LLM Task: "Extract the core event from these articles. What happened? What's the ticker/asset affected? Provide 2-3 sentence summary."
- Output: `{ ticker: string, event_summary: string, affected_entities: string[] }`
- Tools Available: 
  - `fetch_company_context()` â†’ get recent company news for context
  - `extract_entities()` â†’ NER for entity recognition

**Node 3: Sentiment & Impact Analysis**
- Input: Event summary + original articles
- LLM Task: "Analyze the sentiment of these articles about [ticker]. Is this news bullish, bearish, or neutral? What's the likely market impact (high/medium/low)? Explain your reasoning."
- Output: `{ sentiment_score: -1.0 to 1.0, confidence: 0.0 to 1.0, impact_level: "high"|"medium"|"low", reasoning: string }`
- Tools Available:
  - `huggingface_sentiment_model()` â†’ call HF FinBERT as reference point
  - `fetch_historical_reactions()` â†’ how has market reacted to similar news?

**Node 4: Signal Emission**
- Input: All analysis from previous nodes
- Action: Emit to Kafka `processed_signals` topic
- Output: Structured signal (JSON to Kafka)

**Kafka Input Topic:** `raw_news`
**Kafka Output Topic:** `processed_signals`

---

### 2. Investment Agent (LangGraph State Graph)

**Purpose:** Fuse sentiment signals with quantitative data, reason about alignment/contradiction, generate trade recommendations.

**LLM Reasoning Loops:**

**Node 1: Signal Collection**
- Input: Consume from Kafka `processed_signals` + latest quant data
- Action: Fetch quantitative indicators from Binance API
  - RSI(14), SMA(20/50), 24h volume, price momentum
- Output: Aggregated state with sentiment + quant signals

**Node 2: Signal Fusion & Correlation Reasoning**
- Input: News sentiment + quant indicators
- LLM Task: "Analyze these signals:
  - Sentiment: [+0.72 (bullish)]
  - RSI: [58 (neutral)]
  - Volume: [+150% spike]
  - SMA: [price above 20/50 crossover (bullish)]
  
  How do these align? Are there contradictions? What's your overall assessment of the signal strength?"
- Output: `{ signal_alignment: string, contradictions: string[], overall_strength: 0.0 to 1.0 }`
- Tools Available:
  - `calculate_rsi()` â†’ compute RSI
  - `calculate_sma()` â†’ compute SMA
  - `fetch_historical_patterns()` â†’ has this exact signal pattern appeared before?
  - `check_correlation()` â†’ is BTC correlated with ETH movement?

**Node 3: Trade Recommendation Reasoning**
- Input: Signal fusion analysis
- LLM Task: "Given the signal analysis above, should we BUY, SELL, or HOLD [ticker]? What's your confidence level (0-1)? What are the risks and rewards? Size recommendation given portfolio constraints?"
- Output: `{ action: "BUY"|"SELL"|"HOLD", confidence: 0.0 to 1.0, entry_price: float, stop_loss: float, take_profit: float, reasoning: string, risk_factors: string[] }`
- Tools Available:
  - `check_portfolio_limits()` â†’ can we add to this position?
  - `calculate_kelly_criterion()` â†’ optimal position size
  - `fetch_sector_momentum()` â†’ is the sector bullish/bearish?

**Node 4: Emit Trade Signal**
- Action: Send to Kafka `trade_signals` topic

**Kafka Input Topic:** `processed_signals`
**Kafka Output Topic:** `trade_signals`

---

### 3. Personality Agent (LangGraph State Graph)

**Purpose:** Apply investor-archetype reasoning to trades; adjust confidence and decisions through a specific persona lens.

**LLM Reasoning Loops:**

**Node 1: Load Persona Context**
- Input: Current mode (from Redis state store)
- Action: Fetch persona system prompt and instructions
- Output: `{ mode: "buffett"|"soros"|"cathie"|"contrarian", persona_context: string }`

**Node 2: Persona-Specific Reasoning**
- Input: Original trade signal + persona context
- LLM Task (varies by persona):
  - **Buffett Mode:** "You are Warren Buffett. You analyze this trade: [signal]. What would you look for? Is there a margin of safety? Is the company a good business with a durable moat? Adjust confidence based on Buffett principles."
  - **Soros Mode:** "You are George Soros reasoning about reflexivity and market dynamics. This signal shows [sentiment shift]. How do you interpret the market's reaction? Is this a systemic opportunity? Adjust confidence."
  - **Cathie Mode:** "You are Cathie Wood. This signal affects [ticker] in the [sector] space. Does this fit innovation themes (AI, biotech, robotics)? How disruptive is this? Adjust confidence."
  - **Contrarian Mode:** "Market consensus is [bullish/bearish]. You find value in contrarian plays. Does this trade contradict mainstream sentiment in a way that presents opportunity? Adjust confidence."
- Output: `{ confidence_adjustment: float, reasoning: string, recommendation: "accept"|"veto"|"reduce_size" }`
- Tools Available:
  - `fetch_persona_history()` â†’ what would [persona] have done in similar past trades?
  - `check_mainstream_sentiment()` â†’ what's the crowd saying?

**Node 3: Final Decision**
- Input: Persona reasoning result
- LLM Task: "Final decision: Given the persona analysis, should we proceed with this trade as recommended, adjust it, or veto it? Confidence final?"
- Output: `{ final_decision: "BUY"|"SELL"|"HOLD", confidence_adjusted: 0.0 to 1.0, mode: string, adjustments_made: string }`

**Node 4: Emit Validated Trade**
- Action: Send to Kafka `validated_trades` topic

**Kafka Input Topic:** `trade_signals`
**Kafka Output Topic:** `validated_trades`

---

### 4. Research Agent (Optional, LLM-powered)

**Purpose:** For large positions, conduct deep research using LLM reasoning + Parallel API to validate/confirm signal.

**LLM Reasoning Loops:**

**Node 1: Large Trade Filter**
- Input: Validated trade signal
- Condition: If size > $10k threshold, proceed; else pass through

**Node 2: Research Question Formation**
- Input: Trade signal (ticker, reason, size)
- LLM Task: "Formulate 3-5 research questions to validate or invalidate this trade signal. What do we need to know about [company/asset]?"
- Output: `{ research_questions: string[], focus_areas: string[] }`

**Node 3: Call Parallel Research API**
- Input: Research questions + ticker + context
- Action: Call Parallel API with LLM + deep research prompt
- Output: `{ findings: string, confidence: 0.0 to 1.0, sources: string[], contradictions: string[] }`

**Node 4: Research Validation Reasoning**
- Input: Original trade + research findings
- LLM Task: "Based on the research findings, does the original trade signal hold up? Are there contradictions? Should we increase confidence, reduce, or veto?"
- Output: `{ validated: boolean, confidence_delta: float, research_summary: string }`

**Node 5: Emit Researched Trade**
- Action: Send to Kafka `researched_trades` topic

**Kafka Input Topic:** `validated_trades`
**Kafka Output Topic:** `researched_trades`

---

### 5. Broker Agent (Hybrid: Rules + Light LLM)

**Purpose:** Execute trades, track positions, enforce risk limits.

**Logic (Mostly Rules, Optional LLM for Edge Cases):**

**Node 1: Risk Check**
- Input: Trade signal
- Rules-Based Validation:
  - Position size < 10% of portfolio
  - Portfolio correlation < 0.8
  - Max leverage < 2x
  - Max daily drawdown < 5%
- Output: `{ risk_approved: boolean, violations: string[] }`
- Optional LLM: On edge cases, LLM can reason about exceptions

**Node 2: Order Execution**
- Input: Risk-approved trade
- Action: Call Binance API, place LIMIT or MARKET order
- Output: `{ order_id: string, status: string, entry_price: float }`

**Node 3: Position Tracking**
- Input: Filled order
- Action: Log to SQLite, emit to Kafka

**Kafka Input Topic:** `validated_trades` or `researched_trades`
**Kafka Output Topic:** `executed_trades`

---

### 6. Analytics & Dashboard

**Purpose:** Monitor live performance, backtest, visualize signals and LLM reasoning.

**Functions:**
- Consume `executed_trades`, calculate P&L
- Calculate metrics: Sharpe ratio, win rate, max drawdown, profit factor
- Track LLM reasoning quality: reasoning accuracy, decision velocity, confidence calibration
- Streamlit dashboard: live signals, positions, P&L, trade history, agent logs

---

## ğŸ“¡ Data Flow & Kafka Topics

### Kafka Topics Schema (Zod Type Definitions)

**1. `raw_news` (Source)**
```
Topic: raw_news
Retention: 48 hours
Message Type:
  - source: string (reddit, twitter, newsapi, cointelegraph)
  - headline: string
  - content: string
  - url: string
  - published_at: timestamp
  - ticker: string (extracted or heuristic)
```

**2. `processed_signals` (News Agent Output)**
```
Topic: processed_signals
Partitioned by: ticker
Message Type:
  - ticker: string
  - event_id: uuid (deduplicated event)
  - sentiment_score: float [-1.0 to 1.0]
  - confidence: float [0.0 to 1.0]
  - impact_level: enum [high, medium, low]
  - sources: array[string]
  - event_summary: string
  - llm_reasoning: string (full LLM reasoning trace)
  - url: string
  - timestamp: timestamp
```

**3. `trade_signals` (Investment Agent Output)**
```
Topic: trade_signals
Partitioned by: ticker
Message Type:
  - ticker: string
  - action: enum [BUY, SELL, HOLD]
  - confidence: float [0.0 to 1.0]
  - size: float (position size in units or %)
  - entry_price: float
  - stop_loss: float
  - take_profit: float
  - signal_analysis: string (LLM reasoning about signal fusion)
  - risk_factors: array[string]
  - reasoning_trace: string
  - timestamp: timestamp
```

**4. `validated_trades` (Personality Agent Output)**
```
Topic: validated_trades
Partitioned by: ticker
Message Type:
  - trade_signal_id: uuid
  - ticker: string
  - action: enum [BUY, SELL, HOLD]
  - confidence_original: float
  - confidence_adjusted: float
  - mode: string (buffett, soros, cathie, contrarian)
  - persona_reasoning: string (how persona analyzed trade)
  - final_decision: enum [BUY, SELL, HOLD]
  - mode_recommendation: "accept"|"veto"|"reduce_size"
  - reasoning_trace: string
  - timestamp: timestamp
```

**5. `executed_trades` (Broker Agent Output)**
```
Topic: executed_trades
Partitioned by: ticker
Message Type:
  - order_id: string (Binance order ID)
  - ticker: string
  - action: enum [BUY, SELL]
  - size: float
  - entry_price: float
  - fee: float
  - status: enum [filled, partial, rejected]
  - timestamp: timestamp
```

---

## ğŸ§­ Personality Modes (LLM-based)

Each mode is defined by a **system prompt** that guides the LLM's reasoning in the Personality Agent.

### Mode 1: Buffett (Value Investor)

**Philosophy:** Long-term value, margin of safety, fundamentals focus, resist hype.

**Persona System Prompt:**
```
You are Warren Buffett analyzing a potential investment. Your principles:
1. Look for businesses with durable competitive advantages (moats)
2. Demand a margin of safety (buy when price is 30-50% below intrinsic value)
3. Ignore short-term noise; focus on long-term fundamentals
4. Avoid speculative or trendy plays
5. Prefer simple, understandable businesses
6. Strong balance sheet, predictable cash flows

Given the trade signal above:
- Does this company have a moat? Is it simple to understand?
- Is there a margin of safety in the current price?
- Is the trade a speculative play or fundamentals-driven?
- How would you adjust confidence based on these factors?
```

**Confidence Adjustments (LLM determines these):**
- Reduce confidence if: price is at all-time highs, RSI > 70, speculative sector
- Boost confidence if: price down 20%+ from highs, strong fundamentals, margin of safety

---

### Mode 2: Soros (Macro/Momentum)

**Philosophy:** Reflexivity, macro signals, market dynamics, trend-following, exploiting mispricings.

**Persona System Prompt:**
```
You are George Soros analyzing market dynamics and reflexivity. Your approach:
1. Identify inflection points where market perception shifts
2. Follow momentum and price trends (not fight them)
3. React to macro signals: Fed policy, inflation, geopolitical events
4. Exploit market overreactions and herd behavior
5. Exit before the trend reverses

Given the trade signal above:
- Is this a trend reversal or continuation of an existing trend?
- What macro backdrop supports or contradicts this trade?
- Is the market overreacting, creating opportunity?
- How would you adjust confidence and position size?
```

**Confidence Adjustments (LLM determines these):**
- Boost confidence if: volume spike, breakout above 20d high, positive macro catalyst
- Reduce confidence if: trend exhaustion signals, contradicts macro backdrop

---

### Mode 3: Cathie (Innovation-Focused)

**Philosophy:** Tech disruption, biotech, emerging trends, growth-first, disruption thesis.

**Persona System Prompt:**
```
You are Cathie Wood analyzing disruptive innovation themes. Your focus:
1. Identify companies at the forefront of major disruptions (AI, biotech, robotics, energy)
2. Long-term growth potential over near-term profitability
3. Market may underappreciate innovation theses
4. Higher volatility tolerance for high-conviction growth

Given the trade signal above:
- Does this company operate in a disruption theme (AI, blockchain, biotech, renewable)?
- Is the company a leader or innovator in its space?
- What's the long-term TAM (total addressable market)?
- How would you adjust confidence based on innovation potential?
```

**Confidence Adjustments (LLM determines these):**
- Boost confidence if: AI/innovation-related news, disruptive company, high growth
- Reduce confidence if: mature/traditional sector, declining industry

---

### Mode 4: Contrarian

**Philosophy:** Inverse crowd, anti-sentiment, fade consensus, find value in fear.

**Persona System Prompt:**
```
You are a contrarian investor betting against consensus. Your approach:
1. When the crowd is extremely bullish, look for exit signals
2. When the crowd is extremely bearish, look for entry opportunities
3. Find mispricings created by emotional selling
4. Exploit herd behavior

Given the trade signal above:
- What's the mainstream sentiment? (bullish/bearish/neutral)
- How extreme is the sentiment (on a scale 1-10)?
- Does the fundamentals contradict the sentiment?
- Would you increase or decrease confidence based on contrarian positioning?
```

**Confidence Adjustments (LLM determines these):**
- Boost confidence if: market is extremely bearish but fundamentals are OK
- Reduce confidence if: market is euphoric, crowd is all in one direction

---

## ğŸ“Š MVP Scope & Constraints

### In MVP âœ…

| Component | Scope |
|-----------|-------|
| Asset Classes | Crypto (BTC, ETH) â€” 24/7 markets, fast feedback |
| News Sources | Reddit, Twitter, CoinTelegraph, NewsAPI |
| LLM Reasoning | Claude or OpenAI (not local models for quality) |
| Agents | News â†’ Investment â†’ Personality â†’ Broker (all LangGraph) |
| Personality Modes | Buffett, Soros, Cathie, Contrarian (LLM-based personas) |
| Trading Mode | 4-hour candles (not pure 1-min day trading) |
| Broker | Binance paper trading (no real money) |
| Backtesting | Walk-forward on 12 months historical data |
| Dashboard | Streamlit (live metrics, agent reasoning traces) |
| Data Storage | SQLite + Redis + LanceDB (local) |
| Research Agent | Optional; LLM reasoning + Parallel API on large trades |

### Out of MVP âŒ (Phase 2+)

- [ ] Options trading
- [ ] Real-time 1-minute candles
- [ ] Multiple asset classes (stocks, forex, commodities)
- [ ] RL feedback loop for signal weight optimization
- [ ] Distributed deployment (Kubernetes)
- [ ] LLM fine-tuning on historical trading data
- [ ] Multi-model ensemble (combining Claude + GPT-4)
- [ ] Real money trading (staying paper-only)

---

## ğŸ“ Project Structure

```
ai-trading-system/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ package.json                       # Node.js dependencies
â”œâ”€â”€ tsconfig.json                      # TypeScript config
â”œâ”€â”€ .env.example                       # Environment variables template
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.ts                    # API keys, Kafka config, LLM settings
â”‚   â”œâ”€â”€ personalities.ts               # Persona system prompts
â”‚   â””â”€â”€ kafka_schemas.ts               # Zod schema definitions for Kafka topics
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml             # Kafka + Zookeeper + Redis + LanceDB
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.ts              # Abstract LangGraph agent class
â”‚   â”‚   â”œâ”€â”€ news_agent.ts              # LangGraph state graph for news processing
â”‚   â”‚   â”œâ”€â”€ investment_agent.ts        # LangGraph state graph for signal fusion
â”‚   â”‚   â”œâ”€â”€ personality_agent.ts       # LangGraph state graph for persona filtering
â”‚   â”‚   â”œâ”€â”€ broker_agent.ts            # LangGraph for order execution
â”‚   â”‚   â””â”€â”€ research_agent.ts          # Optional: Parallel API + LLM research
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.ts      # LLM tool: sentiment analysis
â”‚   â”‚   â”œâ”€â”€ quant_calculator.ts        # LLM tool: RSI, SMA, volume calculations
â”‚   â”‚   â”œâ”€â”€ deduplicator.ts            # LLM tool: event deduplication
â”‚   â”‚   â”œâ”€â”€ portfolio_validator.ts     # LLM tool: risk checks
â”‚   â”‚   â”œâ”€â”€ parallel_research.ts       # LLM tool: call Parallel API
â”‚   â”‚   â””â”€â”€ historical_lookup.ts       # LLM tool: fetch historical patterns
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”œâ”€â”€ kafka_integration.ts       # Kafka producer/consumer helpers
â”‚   â”‚   â”œâ”€â”€ llm_client.ts              # Claude/OpenAI LLM client wrapper
â”‚   â”‚   â”œâ”€â”€ state_store.ts             # Redis for mode + context persistence
â”‚   â”‚   â””â”€â”€ database.ts                # SQLite + LanceDB access
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ binance_service.ts         # Binance REST API wrapper
â”‚   â”‚   â”œâ”€â”€ news_service.ts            # Reddit, Twitter, NewsAPI aggregators
â”‚   â”‚   â”œâ”€â”€ parallel_service.ts        # Parallel deep research API wrapper
â”‚   â”‚   â””â”€â”€ embeddings_service.ts      # Sentence transformers for dedup
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”œâ”€â”€ kafka_messages.ts          # Zod types for Kafka messages
â”‚   â”‚   â”œâ”€â”€ agent_state.ts             # LangGraph state types
â”‚   â”‚   â””â”€â”€ personalities.ts           # Persona type definitions
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.ts                  # Pino structured logging
â”‚       â”œâ”€â”€ metrics.ts                 # Performance metric calculations
â”‚       â””â”€â”€ reasoning_tracer.ts        # Log LLM reasoning for debugging
â”œâ”€â”€ backtest/
â”‚   â”œâ”€â”€ backtest_engine.ts             # Walk-forward validation harness
â”‚   â”œâ”€â”€ historical_data_loader.ts      # Load historical Binance data
â”‚   â””â”€â”€ signal_replay.ts               # Replay historical signals through agents
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ streamlit_app.py               # Streamlit live monitoring UI
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ news_agent.test.ts
â”‚   â”‚   â”œâ”€â”€ investment_agent.test.ts
â”‚   â”‚   â””â”€â”€ personality_agent.test.ts
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ end_to_end.test.ts
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_kafka.sh                 # Initialize Kafka + topics
â”‚   â”œâ”€â”€ backtest.ts                    # Run backtest
â”‚   â””â”€â”€ run_live_trading.ts            # Start live paper trading
â””â”€â”€ data/
    â”œâ”€â”€ trades.db                      # SQLite: trades, positions, metrics
    â”œâ”€â”€ lancedb/                       # LanceDB: news embeddings for dedup
    â””â”€â”€ backtest_results/              # Historical validation results
```

---

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+ (TypeScript support)
- Docker & Docker Compose (for Kafka, Redis)
- Python 3.10+ (Streamlit dashboard only)
- API Keys:
  - Claude API (Anthropic) or OpenAI API
  - Binance API (paper trading)
  - Reddit app credentials (PRAW)
  - Twitter/X API credentials (tweepy for Node or axios calls)
  - Parallel AI API key (for research agent)

### Setup Steps (High-Level)

1. **Clone & Install**
   ```
   git clone <repo>
   cd ai-trading-system
   npm install
   python -m venv venv && source venv/bin/activate && pip install streamlit
   ```

2. **Configure Environment**
   ```
   cp .env.example .env
   # Edit .env with API keys, Kafka host, LLM model choice (claude-3-5-sonnet or gpt-4)
   ```

3. **Start Infrastructure**
   ```
   docker-compose -f docker/docker-compose.yml up -d
   bash scripts/setup_kafka.sh
   ```

4. **Start Agents (in separate terminals)**
   ```
   npx ts-node src/agents/news_agent.ts
   npx ts-node src/agents/investment_agent.ts
   npx ts-node src/agents/personality_agent.ts
   npx ts-node src/agents/broker_agent.ts
   ```

5. **Launch Dashboard**
   ```
   streamlit run dashboard/streamlit_app.py
   ```

6. **Backtest (Optional)**
   ```
   npx ts-node scripts/backtest.ts
   ```

---

## ğŸ“‹ Phase 1: Detailed Step-by-Step Roadmap (3-4 Weeks)

**Goal:** Build, validate, and paper-trade the complete MVP system with all three agents working end-to-end.

---

### ğŸ”§ STEP 0: Project Setup & Infrastructure (Day 1)

#### **Step 0.1: Create Project Structure**
- [ ] Initialize Node.js monorepo with `apps/backend` and `apps/dashboard`
- [ ] Setup TypeScript config for both
- [ ] Create `.gitignore`, `.env.example`
- [ ] Create folder structure as defined in README

**Checkpoint 0.1:**
```
âœ… Run: npm install â†’ no errors
âœ… Run: npx tsc --noEmit â†’ no TS errors
âœ… Folder structure matches README
```

**Estimated Time:** 30 minutes

---

#### **Step 0.2: Docker Infrastructure (Kafka, Redis, LanceDB)**
- [ ] Create `docker/docker-compose.yml`
  - Kafka + Zookeeper
  - Redis
  - Optional: LanceDB (or use local)
- [ ] Test: `docker-compose up -d`
- [ ] Create Kafka topic setup script

**Checkpoint 0.2:**
```
âœ… docker ps â†’ all containers running
âœ… kafka-topics list â†’ topics exist
âœ… redis-cli ping â†’ PONG
```

**Estimated Time:** 45 minutes

---

#### **Step 0.3: Environment & Configuration**
- [ ] Create `config/settings.ts` with API keys, Kafka hosts
- [ ] Create `.env.example` template
- [ ] Setup logging with Pino

**Checkpoint 0.3:**
```
âœ… cp .env.example .env
âœ… npm run dev â†’ reads .env without errors
âœ… Logs show connection info
```

**Estimated Time:** 20 minutes

---

**ğŸ¯ End of Day 1 Checkpoint:**
```
Project is scaffolded, Docker is running, everything compiles.
No agents built yet, but foundation is solid.
Time invested: ~1.5 hours
```

---

### ğŸ§  STEP 1: Backend Core - LLM & Kafka Integration (Days 2-3)

#### **Step 1.1: LLM Client Wrapper**
- [ ] Create `src/infrastructure/llm_client.ts`
  - Wrapper for Claude or OpenAI API
  - Functions: `invoke()`, `invokeWithTools()`, `streamResponse()`
- [ ] Add error handling and retries
- [ ] Test: Call LLM with simple prompt

**Checkpoint 1.1:**
```
âœ… npm run test:llm-client â†’ LLM responds
âœ… Response contains text/reasoning
âœ… Error handling works (rate limit, timeout)
```

**Estimated Time:** 1 hour

---

#### **Step 1.2: Kafka Producer/Consumer Helpers**
- [ ] Create `src/infrastructure/kafka_producer.ts` (send messages to Kafka)
- [ ] Create `src/infrastructure/kafka_consumer.ts` (listen to topics)
- [ ] Add message validation with Zod schemas
- [ ] Test: Send test message, consume it

**Checkpoint 1.2:**
```
âœ… npm run test:kafka-producer â†’ message sent
âœ… npm run test:kafka-consumer â†’ message received
âœ… Zod validation prevents invalid messages
```

**Estimated Time:** 1 hour

---

#### **Step 1.3: Base Agent Class (LangGraph)**
- [ ] Create `src/agents/base_agent.ts`
  - Abstract class with LangGraph state graph
  - Methods: `process()`, `run()`, `handleError()`
  - Setup Kafka consumer/producer
- [ ] Test: Create simple test agent that logs messages

**Checkpoint 1.3:**
```
âœ… BaseAgent instantiates without errors
âœ… Can attach Kafka topics (input/output)
âœ… Mock agent processes test messages
```

**Estimated Time:** 1.5 hours

---

**ğŸ¯ End of Day 2-3 Checkpoint:**
```
Core infrastructure ready: LLM client, Kafka I/O, Base Agent class.
Ready to build specific agents.
Time invested: ~3.5 hours
```

---

### ğŸ“° STEP 2: News Agent Implementation (Days 4-5)

#### **Step 2.1: News Sources Setup**
- [ ] Create `src/services/news_service.ts`
  - Implement `fetch_reddit_posts()` (PRAW)
  - Implement `fetch_twitter_posts()` (tweepy or axios)
  - Implement `fetch_newsapi()` (NewsAPI)
  - Implement `fetch_cointelegraph()` (RSS or web scrape)
- [ ] Test: Each source returns 5+ articles

**Checkpoint 2.1:**
```
âœ… reddit.fetch() â†’ 5+ posts with text
âœ… twitter.fetch() â†’ 5+ tweets
âœ… newsapi.fetch() â†’ 5+ articles
âœ… All have: headline, URL, source, timestamp
```

**Estimated Time:** 1.5 hours

---

#### **Step 2.2: Deduplication Service**
- [ ] Create `src/services/deduplicator.ts`
  - Use `sentence-transformers` for embeddings
  - Store in LanceDB
  - Function: `isDuplicate(article)` â†’ boolean
- [ ] Test: Feed same headline twice, get dedup flag

**Checkpoint 2.2:**
```
âœ… Similar headlines detected as duplicates
âœ… Different headlines NOT flagged as duplicates
âœ… LanceDB stores embeddings correctly
```

**Estimated Time:** 1 hour

---

#### **Step 2.3: News Agent LLM Nodes**
- [ ] Create `src/agents/news_agent.ts` extending BaseAgent
- [ ] Implement 4 LangGraph nodes:
  1. **Deduplication Node** â†’ LLM: "Are these articles about the same event?"
  2. **Event Extraction Node** â†’ LLM: "What's the core event? Extract ticker."
  3. **Sentiment Analysis Node** â†’ LLM: "Is sentiment bullish/bearish? Impact level?"
  4. **Signal Emission Node** â†’ Emit to Kafka `processed_signals`
- [ ] Test: Feed 5 real news articles

**Checkpoint 2.3:**
```
âœ… Agent processes 5 articles in < 30 seconds
âœ… Sentiment scores are between -1 and 1
âœ… Confidence scores are between 0 and 1
âœ… Kafka messages valid (Zod validation passes)
âœ… LLM reasoning traces logged to SQLite
```

**Estimated Time:** 2.5 hours

---

#### **Step 2.4: API Endpoint for Signals**
- [ ] Create `src/api/signals.ts` (Express route)
  - `GET /api/signals` â†’ list recent signals from SQLite
  - `GET /api/signals/:id` â†’ get full signal + reasoning
- [ ] Test: Curl endpoint, get signals

**Checkpoint 2.4:**
```
âœ… GET /api/signals?limit=10 â†’ returns array
âœ… Each signal has: ticker, sentiment, confidence, timestamp
âœ… Reasoning field contains full LLM trace
```

**Estimated Time:** 45 minutes

---

**ğŸ¯ End of Day 4-5 Checkpoint:**
```
News Agent is LIVE and producing signals.
Every signal has full LLM reasoning.
Time invested: ~5.5 hours
```

---

### ğŸ“Š STEP 3: Investment Agent Implementation (Days 6-7)

#### **Step 3.1: Quantitative Data Fetcher**
- [ ] Create `src/services/binance_service.ts`
  - Function: `fetch_ohlcv(ticker, timeframe)` â†’ Binance API
  - Calculate: RSI(14), SMA(20/50), volume spike, momentum
- [ ] Test: Fetch BTC 4h candles, calculate indicators

**Checkpoint 3.1:**
```
âœ… Binance API returns OHLCV data
âœ… RSI calculated correctly (should be 0-100)
âœ… SMA 20/50 crossover detected
âœ… Volume spike detection works
```

**Estimated Time:** 1 hour

---

#### **Step 3.2: Signal Fusion Engine**
- [ ] Create `src/models/signal_calculator.ts`
  - Function: `fuse_signals()` â†’ combine sentiment + quant
  - Weights: 40% sentiment, 30% RSI, 20% volume, 10% SMA
  - Output: single confidence score
- [ ] Test: Feed sentiment + quant data, get fused score

**Checkpoint 3.2:**
```
âœ… fuse_signals(sentiment=0.8, rsi=58, volume_spike=true) â†’ 0.6-0.8 confidence
âœ… Score is deterministic (same input â†’ same output)
âœ… Weights impact final score
```

**Estimated Time:** 1 hour

---

#### **Step 3.3: Investment Agent LLM Nodes**
- [ ] Create `src/agents/investment_agent.ts` extending BaseAgent
- [ ] Implement 4 LangGraph nodes:
  1. **Signal Collection** â†’ Read from Kafka `processed_signals`, fetch quant data
  2. **Signal Fusion** â†’ LLM: "How do these signals align? Any contradictions?"
  3. **Trade Recommendation** â†’ LLM: "BUY/SELL/HOLD? Confidence? Position size?"
  4. **Signal Emission** â†’ Emit to Kafka `trade_signals`
- [ ] Test: Process 3 signals from News Agent

**Checkpoint 3.3:**
```
âœ… Agent processes 3 signals in < 60 seconds
âœ… Output: { action: "BUY"|"SELL"|"HOLD", confidence: 0-1 }
âœ… Entry price, stop loss, take profit calculated
âœ… Risk factors identified
âœ… Kafka `trade_signals` topic receives messages
```

**Estimated Time:** 2.5 hours

---

#### **Step 3.4: API Endpoint for Trades (Pre-Execution)**
- [ ] Create `src/api/trade_signals.ts`
  - `GET /api/trade-signals` â†’ list pending trades (not yet executed)
- [ ] Test: See trade recommendations before they're executed

**Checkpoint 3.4:**
```
âœ… GET /api/trade-signals â†’ returns pending trades
âœ… Each has: ticker, action, confidence, reasoning
```

**Estimated Time:** 30 minutes

---

**ğŸ¯ End of Day 6-7 Checkpoint:**
```
Investment Agent is LIVE.
News signals â†’ Investment reasoning â†’ Trade recommendations.
Full pipeline from signal to trade decision working.
Time invested: ~5 hours
```

---

### ğŸ­ STEP 4: Personality Agent Implementation (Days 8-9)

#### **Step 4.1: Personality System Prompts**
- [ ] Create `src/config/personalities.ts`
  - Define 4 system prompts (Buffett, Soros, Cathie, Contrarian)
  - Each has detailed persona description
- [ ] Store in Redis for runtime access
- [ ] Test: Load each persona, verify prompts

**Checkpoint 4.1:**
```
âœ… Redis stores all 4 personas
âœ… Can retrieve persona by name
âœ… Prompts are clear and distinct
```

**Estimated Time:** 45 minutes

---

#### **Step 4.2: Mode Persistence**
- [ ] Create `src/infrastructure/state_store.ts` (Redis wrapper)
  - Function: `setMode(mode_name)` â†’ persist to Redis
  - Function: `getMode()` â†’ read from Redis
  - Default mode: "buffett"
- [ ] Test: Set mode, restart agent, verify mode persists

**Checkpoint 4.2:**
```
âœ… setMode("soros") â†’ Redis stores it
âœ… App restart â†’ getMode() returns "soros"
âœ… API can read/write mode
```

**Estimated Time:** 45 minutes

---

#### **Step 4.3: Personality Agent LLM Nodes**
- [ ] Create `src/agents/personality_agent.ts` extending BaseAgent
- [ ] Implement 4 LangGraph nodes:
  1. **Load Persona** â†’ Read mode from Redis, fetch system prompt
  2. **Persona Reasoning** â†’ LLM: "[Persona] analyzes this trade. Thoughts?"
  3. **Decision Adjustment** â†’ LLM: "Final decision: accept/veto/reduce?"
  4. **Validation Emission** â†’ Emit to Kafka `validated_trades`
- [ ] Test: Process trade signal through all 4 personas

**Checkpoint 4.3:**
```
âœ… Each persona produces different confidence adjustment
âœ… Buffett reduces confidence if RSI > 70
âœ… Soros boosts confidence on breakouts
âœ… Cathie boosts on innovation keywords
âœ… Contrarian inverts on extreme sentiment
âœ… Kafka `validated_trades` receives messages
```

**Estimated Time:** 2 hours

---

#### **Step 4.4: Mode Switcher API**
- [ ] Create `src/api/settings.ts`
  - `GET /api/settings` â†’ current mode + config
  - `POST /api/settings/mode` â†’ switch mode
- [ ] Test: Switch between personas, verify behavior changes

**Checkpoint 4.4:**
```
âœ… POST /api/settings/mode { mode: "soros" } â†’ 200 OK
âœ… New trades use Soros reasoning
âœ… Existing positions unaffected
```

**Estimated Time:** 1 hour

---

**ğŸ¯ End of Day 8-9 Checkpoint:**
```
Personality Agent is LIVE.
User can switch between 4 investor personas.
Same signals â†’ different decisions based on mode.
Time invested: ~4.5 hours
```

---

### ğŸ¤– STEP 5: Broker Agent & Order Execution (Days 10-11)

#### **Step 5.1: Broker Agent Risk Checks**
- [ ] Create `src/agents/broker_agent.ts` extending BaseAgent
- [ ] Implement risk validation:
  - Position size < 10% of portfolio
  - Max leverage < 2x
  - Max daily drawdown < 5%
  - Portfolio correlation < 0.8
- [ ] Test: Reject oversized trade, allow normal trade

**Checkpoint 5.1:**
```
âœ… Oversized trade rejected (veto with reason)
âœ… Normal trade approved
âœ… Error logged and sent to Kafka (dead-letter topic)
```

**Estimated Time:** 1.5 hours

---

#### **Step 5.2: Binance Paper Trading Integration**
- [ ] Create `src/services/binance_paper_trading.ts`
  - Implement `place_order(ticker, action, size, price)` for paper mode
  - Track orders in SQLite (order_id, status, entry_price, etc.)
  - Simulate fills at current market price
- [ ] Test: Place BUY order, verify it's tracked in DB

**Checkpoint 5.2:**
```
âœ… place_order("BTC", "BUY", 0.1, 43000) â†’ order_id returned
âœ… SQLite shows order: status="filled", entry_price=43000
âœ… Order ID is valid UUID
```

**Estimated Time:** 1.5 hours

---

#### **Step 5.3: Position Tracking**
- [ ] Create `src/services/position_manager.ts`
  - Track open positions in SQLite: ticker, entry_price, size, mode
  - Calculate unrealized P&L in real-time
  - Update position when price changes
- [ ] Test: Open position, check unrealized P&L

**Checkpoint 5.3:**
```
âœ… Open position in SQLite with: entry_price, size, timestamp, mode
âœ… Current price fetched from Binance
âœ… Unrealized P&L = (current_price - entry_price) * size
âœ… Multiple open positions tracked separately
```

**Estimated Time:** 1 hour

---

#### **Step 5.4: Broker Agent Execution Flow**
- [ ] Consume from Kafka `validated_trades`
- [ ] Run risk checks
- [ ] Place order via Binance
- [ ] Track position in SQLite
- [ ] Emit to Kafka `executed_trades`
- [ ] Test: Full trade execution end-to-end

**Checkpoint 5.4:**
```
âœ… Validated trade consumed
âœ… Risk checks pass/fail
âœ… Order placed on Binance paper trading
âœ… Position created in SQLite
âœ… executed_trades Kafka topic receives confirmation
```

**Estimated Time:** 1 hour

---

**ğŸ¯ End of Day 10-11 Checkpoint:**
```
Broker Agent is LIVE.
Full pipeline: News â†’ Investment â†’ Personality â†’ Broker â†’ Order â†’ Position Tracking.
All 4 agents working end-to-end!
Time invested: ~5 hours
```

---

### ğŸ“¡ STEP 6: Backend API & WebSocket (Days 12-13)

#### **Step 6.1: Dashboard API Endpoints**
- [ ] Create `src/api/dashboard.ts`
  - `GET /api/dashboard` â†’ portfolio summary, metrics, recent trades, agent status
- [ ] Create `src/api/trades.ts` â†’ GET /api/trades (list all)
- [ ] Create `src/api/positions.ts` â†’ GET /api/positions (current)
- [ ] Test: All endpoints return valid JSON

**Checkpoint 6.1:**
```
âœ… GET /api/dashboard â†’ { portfolio, metrics, recentTrades, agentStatus }
âœ… GET /api/trades?limit=10 â†’ array of trades
âœ… GET /api/positions â†’ current positions with unrealized P&L
```

**Estimated Time:** 1.5 hours

---

#### **Step 6.2: WebSocket Setup**
- [ ] Create `src/infrastructure/websocket_server.ts`
  - Listen to Kafka topics (executed_trades, processed_signals, position updates)
  - Broadcast to connected clients
- [ ] Setup Socket.io on Express
- [ ] Test: Connect WebSocket client, receive live updates

**Checkpoint 6.2:**
```
âœ… WebSocket server starts without errors
âœ… Client connects: ws://localhost:3000/ws
âœ… New trade â†’ broadcasts to all clients in < 100ms
âœ… Message format: { type: "trade_executed", data: {...} }
```

**Estimated Time:** 1.5 hours

---

#### **Step 6.3: Backend Server Setup**
- [ ] Create `src/main.ts` (Express entry point)
  - Setup routes, middleware, error handling
  - Start API server on port 3000
  - Start WebSocket server
- [ ] Test: `npm run start` â†’ server runs

**Checkpoint 6.3:**
```
âœ… npm run start â†’ server starts
âœ… curl http://localhost:3000/api/dashboard â†’ 200 OK
âœ… WebSocket available at ws://localhost:3000/ws
```

**Estimated Time:** 1 hour

---

**ğŸ¯ End of Day 12-13 Checkpoint:**
```
Backend is feature-complete!
All APIs working, WebSocket streaming live data.
Ready for frontend to connect.
Time invested: ~4 hours
```

---

### ğŸ¨ STEP 7: Frontend Setup & Dashboard Page (Days 14-15)

#### **Step 7.1: Next.js Project Setup**
- [ ] Create Next.js project in `apps/dashboard`
- [ ] Install: Shadcn/ui, TailwindCSS, Recharts, React Query, Socket.io
- [ ] Setup TypeScript, eslint, prettier
- [ ] Test: `npm run dev` â†’ app runs on localhost:3001

**Checkpoint 7.1:**
```
âœ… Next.js app compiles without errors
âœ… Shadcn/ui components available
âœ… Tailwind CSS working
âœ… Development server running
```

**Estimated Time:** 1.5 hours

---

#### **Step 7.2: Layout Components**
- [ ] Create `components/layout/Header.tsx` (top nav)
- [ ] Create `components/layout/Sidebar.tsx` (left nav with links)
- [ ] Create root `app/layout.tsx` (wraps all pages)
- [ ] Test: Sidebar and header visible on all pages

**Checkpoint 7.2:**
```
âœ… Header displays app title + current mode
âœ… Sidebar has links: Dashboard, Trades, Positions, Signals, Settings
âœ… Active link highlighted
```

**Estimated Time:** 1 hour

---

#### **Step 7.3: Dashboard Widgets**
- [ ] Create `components/dashboard/PortfolioCard.tsx` (P&L summary)
- [ ] Create `components/dashboard/MetricsCard.tsx` (Sharpe, win rate)
- [ ] Create `components/dashboard/AgentStatusWidget.tsx` (agent health)
- [ ] Create `components/dashboard/RecentTradesWidget.tsx` (last 5 trades)
- [ ] Test: Widgets render without data (static mockups)

**Checkpoint 7.3:**
```
âœ… All widgets render on page
âœ… Widgets have placeholder data
âœ… Layout looks clean (Shadcn/ui styling)
```

**Estimated Time:** 2 hours

---

#### **Step 7.4: API Client & Data Fetching**
- [ ] Create `lib/api-client.ts` (fetch wrapper)
- [ ] Create `hooks/useDashboard.ts` (React Query for dashboard data)
- [ ] Connect Dashboard page to backend
- [ ] Test: Fetch real data from backend

**Checkpoint 7.4:**
```
âœ… useDashboard() hook fetches dashboard data
âœ… Portfolio card shows real total balance
âœ… Recent trades populated from database
âœ… Metrics show calculated Sharpe, win rate
```

**Estimated Time:** 1.5 hours

---

#### **Step 7.5: Dashboard Home Page**
- [ ] Create `app/page.tsx` (main dashboard)
- [ ] Assemble all widgets
- [ ] Add basic styling
- [ ] Test: Page loads, shows real data

**Checkpoint 7.5:**
```
âœ… Dashboard page loads without errors
âœ… All 5 widgets visible
âœ… Real backend data displayed
âœ… Page refreshes on manual reload
```

**Estimated Time:** 1 hour

---

**ğŸ¯ End of Day 14-15 Checkpoint:**
```
Frontend dashboard page LIVE with real backend data.
User can see portfolio, metrics, recent trades.
Time invested: ~7.5 hours
```

---

### ğŸ“ˆ STEP 8: WebSocket Real-Time Updates (Days 16-17)

#### **Step 8.1: WebSocket Client Hook**
- [ ] Create `hooks/useWebSocket.ts`
  - Connect to `ws://localhost:3000/ws`
  - Listen for: trade_executed, position_update, signal_received, metrics_update
  - Auto-reconnect on disconnect
- [ ] Test: Console logs incoming WebSocket messages

**Checkpoint 8.1:**
```
âœ… WebSocket connects: ws://... 
âœ… Receives: { type: "trade_executed", data: {...} }
âœ… Console shows incoming messages
âœ… Auto-reconnects after 5-second disconnect
```

**Estimated Time:** 1 hour

---

#### **Step 8.2: Real-Time Dashboard Updates**
- [ ] Modify `components/dashboard/RecentTradesWidget.tsx`
  - Subscribe to `trade_executed` via WebSocket
  - New trades appear at top of list without refresh
- [ ] Modify `components/dashboard/MetricsCard.tsx`
  - Subscribe to `metrics_update`
  - P&L updates in real-time
- [ ] Test: Open dashboard, watch trades appear live

**Checkpoint 8.2:**
```
âœ… Place order via CLI on backend
âœ… Dashboard instantly shows new trade (no refresh needed)
âœ… P&L updates every 2-5 seconds
âœ… Portfolio card updates with new balance
```

**Estimated Time:** 1.5 hours

---

#### **Step 8.3: Trades History Page**
- [ ] Create `app/trades/page.tsx`
- [ ] Create `components/trades/TradesTable.tsx` (sortable, filterable)
- [ ] Create `components/trades/TradeDetailsModal.tsx` (click row â†’ full details + reasoning)
- [ ] Connect to `/api/trades` endpoint
- [ ] Test: List all trades, click to see full reasoning

**Checkpoint 8.3:**
```
âœ… Trades table loads 20 rows
âœ… Click row â†’ modal shows full trade details
âœ… Modal displays full LLM reasoning from all agents
âœ… Table sortable by any column
```

**Estimated Time:** 2 hours

---

#### **Step 8.4: Live Signals Page**
- [ ] Create `app/signals/page.tsx`
- [ ] Create `components/signals/SignalsTable.tsx` (real-time stream)
- [ ] Subscribe to WebSocket `signal_received`
- [ ] New signals appear at top
- [ ] Color coding (green = bullish, red = bearish, yellow = hold)
- [ ] Test: Watch signals stream live as News Agent processes articles

**Checkpoint 8.4:**
```
âœ… Signals page opens
âœ… New signals appear in real-time (WebSocket)
âœ… Signals color-coded by sentiment
âœ… Can expand row to see LLM reasoning
âœ… Filters by ticker, agent, signal type work
```

**Estimated Time:** 2 hours

---

**ğŸ¯ End of Day 16-17 Checkpoint:**
```
Frontend is LIVE and reactive!
Real-time updates flowing from backend via WebSocket.
Time invested: ~6.5 hours
```

---

### âš™ï¸ STEP 9: Remaining Pages & Settings (Days 18-19)

#### **Step 9.1: Positions Page**
- [ ] Create `app/positions/page.tsx`
- [ ] Create `components/positions/PositionsTable.tsx`
- [ ] Real-time P&L updates via WebSocket
- [ ] Quick actions: close position, adjust stops
- [ ] Test: See current positions with live P&L

**Checkpoint 9.1:**
```
âœ… Positions page loads
âœ… Shows all open positions with entry/current price
âœ… Unrealized P&L updates every 2-5 seconds
âœ… Close position button works
```

**Estimated Time:** 1.5 hours

---

#### **Step 9.2: Settings Page**
- [ ] Create `app/settings/page.tsx`
- [ ] Create `components/settings/ModeSelector.tsx` (Buffett/Soros/Cathie/Contrarian)
- [ ] Create `components/settings/AgentMonitor.tsx` (agent status + logs)
- [ ] Connect to `/api/settings` endpoints
- [ ] Test: Switch modes, see status changes

**Checkpoint 9.2:**
```
âœ… Settings page loads
âœ… Mode selector shows 4 options
âœ… Switch mode â†’ persists to backend (Redis)
âœ… Agent status shows real-time health
âœ… System logs visible
```

**Estimated Time:** 1.5 hours

---

#### **Step 9.3: Error Handling & Loading States**
- [ ] Add error boundaries to all pages
- [ ] Add loading spinners during data fetch
- [ ] Handle WebSocket disconnects gracefully
- [ ] Show toast notifications for errors/success
- [ ] Test: Simulate network errors, see graceful handling

**Checkpoint 9.3:**
```
âœ… Network error â†’ shows error toast
âœ… Slow load â†’ loading spinner appears
âœ… WebSocket disconnects â†’ shows "reconnecting..." message
âœ… No console errors
```

**Estimated Time:** 1 hour

---

**ğŸ¯ End of Day 18-19 Checkpoint:**
```
Frontend feature-complete!
All 5 pages working: Dashboard, Trades, Positions, Signals, Settings.
Real-time updates flowing smoothly.
Time invested: ~4 hours
```

---

### ğŸ§ª STEP 10: Integration Testing & Validation (Days 20-21)

#### **Step 10.1: End-to-End Test (Full Pipeline)**
- [ ] Start all backend agents
- [ ] Manually post news article to Kafka `raw_news`
- [ ] Watch pipeline: News Agent â†’ Investment Agent â†’ Personality Agent â†’ Broker Agent
- [ ] Verify trade appears in dashboard in real-time
- [ ] Test with 3 different news items + 3 different modes

**Checkpoint 10.1:**
```
âœ… Post article to Kafka
âœ… News Agent processes (sentiment + dedup)
âœ… Investment Agent generates trade signal
âœ… Personality Agent adjusts confidence
âœ… Broker Agent places order
âœ… Dashboard shows new trade in < 10 seconds
âœ… All LLM reasoning traces logged and visible in UI
```

**Estimated Time:** 2 hours

---

#### **Step 10.2: Error Handling & Recovery**
- [ ] Test: Stop an agent, verify error shown in settings
- [ ] Test: Restart agent, system recovers
- [ ] Test: Invalid Kafka message (bad format) â†’ dead-letter queue
- [ ] Test: LLM API timeout â†’ retry + backoff
- [ ] Verify: All errors logged to SQLite + visible in debug page

**Checkpoint 10.2:**
```
âœ… Agent failure detected within 10 seconds
âœ… Dashboard shows ğŸ”´ for failed agent
âœ… Restart agent â†’ system recovers
âœ… Dead-letter queue contains failed messages
âœ… Logs searchable and timestamped
```

**Estimated Time:** 1.5 hours

---

#### **Step 10.3: Performance Baseline**
- [ ] Measure: Signal latency (news â†’ trade decision)
  - Target: < 5 minutes end-to-end
- [ ] Measure: Dashboard update latency
  - Target: < 1 second after trade execution
- [ ] Measure: API response times
  - Target: < 500ms for dashboard, signals
- [ ] Document baseline metrics

**Checkpoint 10.3:**
```
âœ… News â†’ Trade decision latency: 2-4 minutes
âœ… Trade appears on dashboard: 0.5 seconds
âœ… API responds in < 300ms
âœ… WebSocket broadcasts in < 100ms
```

**Estimated Time:** 1 hour

---

**ğŸ¯ End of Day 20-21 Checkpoint:**
```
MVP is stable and tested!
All components integrated and working together.
Ready for paper trading validation.
Time invested: ~4.5 hours
```

---

### ğŸ“Š STEP 11: Paper Trading Validation (Days 22-28: 1 Week Live)

#### **Step 11.1: Paper Trading Start**
- [ ] Start all agents
- [ ] Run live for 24 hours, monitor for errors
- [ ] Collect initial metrics

**Checkpoint 11.1:**
```
âœ… No crashes in 24 hours
âœ… All agents processing signals
âœ… Orders executing on Binance paper account
âœ… Dashboard stable and responsive
```

**Estimated Time:** 1 day observation

---

#### **Step 11.2: Daily Metrics Tracking**
- [ ] Track daily: # trades, win rate, P&L, Sharpe ratio
- [ ] End of Week 1:
  - Total trades: 10+
  - Win rate: 50%+?
  - Sharpe ratio: > 0.5?
  - Max drawdown: < 15%?

**Checkpoint 11.2:**
```
âœ… If metrics are green (Sharpe > 0.5, win rate > 50%):
   â†’ Concept is validated! Move to Phase 2.
   
âœ… If metrics are yellow (mixed results):
   â†’ Debug which agent is underperforming.
   â†’ Tune LLM prompts or signal weights.
   â†’ Run another week.
   
âŒ If metrics are red (Sharpe < 0.3, win rate < 40%):
   â†’ Revisit agent reasoning and LLM prompts.
   â†’ Check for signal lag or over-trading.
   â†’ Retry after fixes.
```

**Estimated Time:** 1 week observation + analysis

---

**ğŸ¯ End of Week (Days 22-28):**
```
Phase 1 MVP complete!
Live paper trading data collected.
Decision point: Phase 2 or debug?
Time invested: ~55 hours total
```

---

## ğŸ“‹ Phase 1 Checkpoint Summary

| Day(s) | Step | What Gets Built | Checkpoint |
|--------|------|-----------------|-----------|
| 1 | 0 | Project structure + Docker | Infrastructure ready |
| 2-3 | 1 | LLM client + Kafka + Base Agent | Core utilities working |
| 4-5 | 2 | News Agent + API | First agent live, signals flowing |
| 6-7 | 3 | Investment Agent | Signal fusion working |
| 8-9 | 4 | Personality Agent + Mode switching | All 3 agents integrated |
| 10-11 | 5 | Broker Agent + Order execution | Full pipeline working! |
| 12-13 | 6 | Backend API + WebSocket | Backend feature-complete |
| 14-15 | 7 | Dashboard page + widgets | Frontend MVP page live |
| 16-17 | 8 | Real-time updates | WebSocket streaming |
| 18-19 | 9 | Remaining pages + settings | Frontend complete |
| 20-21 | 10 | E2E testing + validation | System stable |
| 22-28 | 11 | Paper trading week | Metrics collected |

**Total: ~55 hours over 4 weeks**

---

## âœ… How to Use This Roadmap

1. **Pick a step** (e.g., Step 2.1: News Sources)
2. **Build it** (1-2 hours usually)
3. **Hit the checkpoint** (test it)
4. **Celebrate!** âœ… (checkpoint passed)
5. **Move to next step**

Each step has:
- Clear deliverables
- Specific test criteria
- Estimated time
- No surprises!

---

## ğŸš¨ If You Get Stuck

For any step:
1. Check the checkpoint criteria (did you meet all of them?)
2. Re-read the step description
3. Check existing code examples (check similar files already built)
4. Test each piece independently before combining
5. If stuck >30 min, document the issue and move on (return later)

---

**Ready to start? Pick Step 0.1 and let's go!** ğŸš€

---

## ğŸ§  LLM Reasoning Visibility

One key feature of this MVP: **trace LLM reasoning for every decision**. Each agent logs:
- Input data (signals, context)
- LLM system prompt used
- LLM response (reasoning, analysis)
- Final decision made
- Confidence and adjustments

This is logged to SQLite and displayed in Streamlit for debugging and validation.

---

## ğŸ“ˆ Future Phases

### Phase 2: Hybrid Reasoning & Tool Improvement
- [ ] Add more sophisticated LLM tools (e.g., sector comparison, insider buying analysis)
- [ ] Implement dynamic persona weighting (blend modes based on market regime)
- [ ] Add RL-based feedback: learn from trade outcomes, improve prompts

### Phase 3: Multi-Asset & Advanced Reasoning
- [ ] Expand to US large-cap stocks
- [ ] Add options (covered calls, protective puts)
- [ ] Multi-model reasoning (ensemble of Claude + GPT-4 for better accuracy)

### Phase 4: Production Hardening
- [ ] Distributed deployment (Kubernetes)
- [ ] Real-time monitoring & alerting
- [ ] LLM fine-tuning on historical trading data
- [ ] Advanced risk management (correlation hedging, dynamic position sizing)

### Phase 5: Autonomous Learning
- [ ] Store all trades + LLM reasoning in vector DB
- [ ] Continuous prompt optimization based on performance
- [ ] Learn when to switch personas automatically based on market regime

---

## ğŸ“‹ Frontend Architecture

### Technology Stack for UI/Dashboard

| Layer | Tool | Purpose | Why |
|-------|------|---------|-----|
| **Framework** | Next.js 14+ (App Router) | Full-stack React + routing | Type-safe, fast, production-grade |
| **UI Library** | Shadcn/ui + TailwindCSS | Component library + styling | Beautiful, accessible, zero-config |
| **Real-Time Updates** | WebSocket (Socket.io) | Live signals, P&L streaming | Low-latency, efficient |
| **Charts** | Recharts | Financial charts & graphs | Built for trading data |
| **State Management** | TanStack Query (React Query) | Server state sync | Handles real-time updates cleanly |
| **Tables** | TanStack Table | Trade history, positions | Sortable, filterable, responsive |
| **Notifications** | Sonner | Trade alerts, system notifications | Toast UI for events |
| **Backend API** | Express/NestJS (same TypeScript codebase) | REST + WebSocket endpoints | Seamless integration |

---

### Frontend Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BACKEND (Node.js/LangGraph Agents)                         â”‚
â”‚  â”œâ”€ News Agent â†’ Kafka processed_signals                    â”‚
â”‚  â”œâ”€ Investment Agent â†’ Kafka trade_signals                  â”‚
â”‚  â”œâ”€ Personality Agent â†’ Kafka validated_trades             â”‚
â”‚  â””â”€ Broker Agent â†’ Kafka executed_trades                   â”‚
â”‚     â†“                                                       â”‚
â”‚  Backend API Server (Express/NestJS)                       â”‚
â”‚  â”œâ”€ REST endpoints: /api/trades, /api/positions, etc.      â”‚
â”‚  â”œâ”€ WebSocket: /ws/live (real-time updates)               â”‚
â”‚  â””â”€ Kafka Consumer: subscribes to all topics              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FRONTEND (Next.js React Dashboard)                         â”‚
â”‚  â”œâ”€ Dashboard Page (P&L, Portfolio, Recent Trades)         â”‚
â”‚  â”œâ”€ Live Signals Page (Real-time signal stream)            â”‚
â”‚  â”œâ”€ Trades History Page (Search, filter, view reasoning)   â”‚
â”‚  â”œâ”€ Positions Page (Current holdings, unrealized P&L)      â”‚
â”‚  â”œâ”€ Settings Page (Mode switcher, config)                  â”‚
â”‚  â””â”€ Agent Debug Page (Reasoning traces, logs)              â”‚
â”‚     â†“                                                       â”‚
â”‚  WebSocket Connection                                       â”‚
â”‚  â”œâ”€ Listens: executed_trades, signals, position updates   â”‚
â”‚  â””â”€ Updates: Real-time charts, tables, metrics            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Frontend Features (MVP)

#### **1. Dashboard Home Page**

**Purpose:** At-a-glance overview of trading system status and performance.

**Components:**
- **Portfolio Summary Card**
  - Total balance (USD equivalent)
  - Today's P&L (absolute + %)
  - All-time P&L
  
- **Portfolio Allocation Chart**
  - Pie chart: BTC %, ETH %, USD cash %
  - Holdings summary
  
- **Performance Metrics Widget**
  - Sharpe ratio (risk-adjusted returns)
  - Win rate (% profitable trades)
  - Max drawdown (worst peak-to-trough)
  - Profit factor (avg win / avg loss)
  - Total trades (lifetime)
  
- **Current Mode Display**
  - Active personality mode (Buffett/Soros/Cathie/Contrarian)
  - Quick mode switcher button
  - Mode description tooltip
  
- **Agent Status Panel**
  - ğŸŸ¢/ğŸ”´ status for each agent (News, Investment, Personality, Broker)
  - Last heartbeat timestamp
  - Error indicator if agent down
  
- **Recent Trades Widget**
  - Last 5 trades (newest first)
  - Columns: Ticker | Action | Entry | Current | P&L | Timestamp
  - Expandable rows â†’ view full LLM reasoning
  - Color-coded (green = profit, red = loss)
  
- **Live Signals Widget**
  - Last 3-5 signals being processed
  - Real-time updates via WebSocket
  - Shows sentiment score, confidence, impact level

**Real-Time Updates:** WebSocket subscription to `executed_trades`, `processed_signals`, position updates (every 2-5 sec)

---

#### **2. Live Signals Page**

**Purpose:** Monitor real-time news sentiment processing and investment reasoning.

**Components:**
- **Signals Stream Table**
  - Columns: Timestamp | Agent | Ticker | Sentiment | Confidence | Signal Type | Status
  - Newest first (reverse chronological)
  - Rows color-coded: Green (bullish), Red (bearish), Yellow (neutral/hold)
  - Expandable rows â†’ show full LLM reasoning trace
  
- **Filters**
  - By ticker (BTC, ETH, etc.)
  - By agent (News Agent, Investment Agent, Personality Agent)
  - By signal type (BUY, SELL, HOLD)
  - By confidence (threshold slider)
  
- **Reasoning Trace Modal (Expandable Row)**
  - Input data to agent
  - LLM system prompt used
  - Full LLM response / reasoning
  - Final decision + confidence
  - Applicable tools called
  
- **Signal Statistics**
  - Total signals in last hour / day
  - Sentiment distribution (bullish % / bearish % / neutral %)
  - Average confidence level
  - Most active ticker

**Real-Time Updates:** WebSocket to `processed_signals`, `trade_signals`, `validated_trades` topics

---

#### **3. Trades History Page**

**Purpose:** Browse, search, filter all historical trades with full reasoning traces.

**Components:**
- **Trades Data Table**
  - Columns: Order ID | Ticker | Action | Entry Price | Exit Price | Size | Entry Time | Exit Time | P&L | Mode | Status
  - Sortable by any column
  - Filterable by ticker, mode, status
  - Searchable by order ID
  - Pagination (20 rows/page)
  
- **Trade Details Modal (Click Row)**
  - Order ID, ticker, action, entry/exit prices
  - Entry time, exit time, holding duration
  - Unrealized vs realized P&L
  - Mode trade was opened in
  - **Full reasoning trace:**
    - News signal that triggered trade
    - Investment agent's signal fusion reasoning
    - Personality agent's decision process
    - Broker agent's execution notes
  - Historical price chart at entry point
  - Exit reason (if closed)
  
- **Statistics Panel**
  - Total trades (lifetime)
  - Win rate
  - Average win / Average loss
  - Largest win / Largest loss
  - Trades by mode (Buffett: X, Soros: Y, etc.)
  
- **Filters & Search**
  - By date range
  - By ticker
  - By mode
  - By status (open, closed)
  - Profitable vs unprofitable only

**Data Source:** API endpoint `/api/trades` with pagination + filters. Real-time updates via WebSocket for new trades.

---

#### **4. Positions Page**

**Purpose:** View current open positions and unrealized P&L.

**Components:**
- **Positions Table / Card View**
  - Columns (if table): Ticker | Entry Price | Current Price | Size | Unrealized P&L | Entry Time | Mode | Actions
  - Alternative card view: one card per position
  - Color-coded (green = profit, red = loss)
  - Real-time price updates via WebSocket
  
- **Position Details Card (Click Row/Card)**
  - Ticker details
  - Entry signal (link to that trade)
  - Current sentiment for this ticker
  - Entry price, current price, change %
  - Position size (units + USD value)
  - Unrealized P&L (USD + %)
  - Entry time, days held
  - Mode opened in
  - Stop loss / Take profit targets (if set)
  
- **Quick Actions** (buttons on each position)
  - Close position (market order)
  - Adjust stop loss / take profit
  - Add to position
  - View reasoning (link to LLM trace)
  
- **Portfolio Summary**
  - Total open positions
  - Total unrealized P&L
  - Portfolio concentration (largest position %)
  - Average entry price vs current price

**Real-Time Updates:** WebSocket to position P&L changes, current prices, sentiment updates (every 2-5 sec)

---

#### **5. Settings Page**

**Purpose:** Configure trading system and view agent status.

**Components:**
- **Mode Selector**
  - Radio buttons or segmented control: Buffett | Soros | Cathie | Contrarian
  - Description of each mode
  - "Switch Mode" button (affects only new trades, not existing positions)
  - Display current mode highlight
  
- **Portfolio Configuration**
  - Max position size (% of portfolio, e.g., 20%)
  - Max leverage ratio (e.g., 2x)
  - Max daily drawdown trigger (%, e.g., 5%)
  - Min confidence threshold for trades (e.g., 0.5)
  
- **Agent Monitoring**
  - Agent status table: Agent | Status | Last Heartbeat | Last Error
  - ğŸŸ¢ Running | ğŸ”´ Stopped | ğŸŸ¡ Error
  - View logs button for each agent
  - Restart agent button
  
- **System Logs (Recent)**
  - Last 20 errors/warnings
  - Timestamp | Level | Agent | Message
  - Color-coded (red = error, yellow = warning, blue = info)
  
- **API Configuration** (if needed)
  - Test Kafka connection
  - Test Binance API
  - Test LLM API (Claude/OpenAI)

---

#### **6. Agent Reasoning Debugger (Internal Page)**

**Purpose:** Debug LLM reasoning traces, test prompts, inspect agent decisions.

**Components:**
- **Reasoning Traces Search/Filter**
  - List all recent LLM reasoning traces
  - Searchable by agent, ticker, timestamp
  - Filterable by agent type
  - Export traces as JSON
  
- **Trace Details (Expandable Row)**
  - Agent name
  - Input data (context, signals, quant data)
  - LLM system prompt used
  - LLM response (reasoning)
  - Tools called + results
  - Final decision + confidence
  - Timestamp
  
- **Manual LLM Testing** (textarea)
  - Input custom market data
  - Select agent to test
  - Submit â†’ see LLM reasoning in real-time
  - Useful for prompt tuning

- **Agent Performance Trend**
  - Decision accuracy rate over time
  - Confidence calibration (is 80% confidence actually right 80% of the time?)
  - Signal lag (time from signal to trade)

---

### Backend API Endpoints (for Frontend)

```
Authentication (if needed):
GET    /api/auth/status          â†’ { authenticated: boolean }

Dashboard:
GET    /api/dashboard             â†’ { portfolio, metrics, recent_trades, agent_status }

Trades:
GET    /api/trades?limit=20&offset=0          â†’ { trades: [...], total: number }
GET    /api/trades/:id            â†’ { full trade details + reasoning }
POST   /api/trades/:id/close      â†’ { order_id, status }

Positions:
GET    /api/positions             â†’ { positions: [...], summary: {...} }
GET    /api/positions/:ticker     â†’ { position details }
POST   /api/positions/:ticker/close â†’ close position

Signals:
GET    /api/signals?limit=50      â†’ { signals: [...], stats: {...} }
GET    /api/signals/:id           â†’ { signal details + full trace }

Settings:
GET    /api/settings              â†’ { mode, portfolio_config, api_status }
POST   /api/settings/mode         â†’ { mode: "buffett" }
POST   /api/settings/config       â†’ { max_position_size, max_leverage, etc. }

Agents:
GET    /api/agents/status         â†’ { agents: [...], logs: [...] }
GET    /api/agents/:name/logs     â†’ { logs: [...] }

Reasoning (Debug):
GET    /api/debug/reasoning?limit=100  â†’ { traces: [...] }
GET    /api/debug/reasoning/:id        â†’ { full trace details }
POST   /api/debug/test-llm             â†’ test LLM reasoning

Real-Time WebSocket:
WS     /ws/live
       Subscribe messages:
       - "signals" â†’ new signals from News Agent
       - "trades" â†’ new trades from Broker Agent
       - "positions" â†’ position P&L updates
       - "metrics" â†’ portfolio metrics updates
       
       Server sends:
       { type: "signal", data: {...} }
       { type: "trade", data: {...} }
       { type: "position_update", data: {...} }
       { type: "metrics_update", data: {...} }
```

---

### Frontend Component Structure

```
Dashboard (Next.js App Router):
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx                    # Root layout with providers
â”‚   â”œâ”€â”€ page.tsx                      # Dashboard home (/dashboard)
â”‚   â”œâ”€â”€ trades/
â”‚   â”‚   â”œâ”€â”€ page.tsx                  # Trade history (/trades)
â”‚   â”‚   â””â”€â”€ [id].tsx                  # Trade details (/trades/:id)
â”‚   â”œâ”€â”€ positions/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Current positions (/positions)
â”‚   â”œâ”€â”€ signals/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Live signals (/signals)
â”‚   â”œâ”€â”€ settings/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Settings (/settings)
â”‚   â””â”€â”€ debug/
â”‚       â””â”€â”€ page.tsx                  # Reasoning debugger (/debug)

Components:
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ PortfolioCard.tsx             # Portfolio summary + chart
â”‚   â”œâ”€â”€ MetricsCard.tsx               # Sharpe, win rate, drawdown
â”‚   â”œâ”€â”€ AgentStatusWidget.tsx         # Agent status indicators
â”‚   â”œâ”€â”€ RecentTradesWidget.tsx        # Last 5 trades
â”‚   â””â”€â”€ LiveSignalsWidget.tsx         # Live signal feed
â”‚
â”œâ”€â”€ trades/
â”‚   â”œâ”€â”€ TradesTable.tsx               # Trade history table
â”‚   â”œâ”€â”€ TradeDetailsModal.tsx         # Full trade + reasoning
â”‚   â”œâ”€â”€ ReasoningTraceView.tsx        # LLM reasoning display
â”‚   â””â”€â”€ TradeFilters.tsx              # Filters + search
â”‚
â”œâ”€â”€ positions/
â”‚   â”œâ”€â”€ PositionsTable.tsx            # Current positions
â”‚   â”œâ”€â”€ PositionCard.tsx              # Card view of position
â”‚   â”œâ”€â”€ PositionDetailsModal.tsx      # Position details
â”‚   â””â”€â”€ QuickActionsMenu.tsx          # Close, adjust SL/TP
â”‚
â”œâ”€â”€ signals/
â”‚   â”œâ”€â”€ SignalsTable.tsx              # Signal stream
â”‚   â”œâ”€â”€ SignalFilters.tsx             # Filter by ticker, agent, type
â”‚   â””â”€â”€ ReasoningTraceModal.tsx       # Expand and view reasoning
â”‚
â”œâ”€â”€ settings/
â”‚   â”œâ”€â”€ ModeSelector.tsx              # Buffett/Soros/Cathie/Contrarian picker
â”‚   â”œâ”€â”€ PortfolioConfig.tsx           # Position sizing, leverage config
â”‚   â”œâ”€â”€ AgentMonitor.tsx              # Agent status table
â”‚   â””â”€â”€ SystemLogs.tsx                # Recent errors/warnings
â”‚
â”œâ”€â”€ debug/
â”‚   â”œâ”€â”€ ReasoningTraceBrowser.tsx     # Search & browse traces
â”‚   â”œâ”€â”€ LLMTestPanel.tsx              # Manual LLM testing
â”‚   â””â”€â”€ PerformanceTrend.tsx          # Decision accuracy chart
â”‚
â”œâ”€â”€ layout/
â”‚   â”œâ”€â”€ Header.tsx                    # Top navigation bar
â”‚   â”œâ”€â”€ Sidebar.tsx                   # Left navigation
â”‚   â””â”€â”€ TopBar.tsx                    # Secondary info bar
â”‚
â””â”€â”€ ui/                               # Shadcn components
    â”œâ”€â”€ Button.tsx
    â”œâ”€â”€ Card.tsx
    â”œâ”€â”€ Table.tsx
    â”œâ”€â”€ Modal.tsx
    â”œâ”€â”€ Badge.tsx
    â”œâ”€â”€ Toast.tsx
    â””â”€â”€ ... (all shadcn components)

Hooks:
â”œâ”€â”€ useWebSocket.ts                   # WebSocket connection hook
â”œâ”€â”€ useTradeHistory.ts                # Fetch trade history + filters
â”œâ”€â”€ usePositions.ts                   # Fetch current positions
â”œâ”€â”€ useSignals.ts                     # Subscribe to signals
â”œâ”€â”€ useMetrics.ts                     # Fetch portfolio metrics
â”œâ”€â”€ useDashboard.ts                   # Dashboard data aggregation
â””â”€â”€ useReasoningTraces.ts             # Fetch LLM reasoning traces

Lib:
â”œâ”€â”€ api-client.ts                     # Fetch client with auth
â”œâ”€â”€ websocket-client.ts               # WebSocket wrapper
â”œâ”€â”€ types.ts                          # Shared types with backend
â””â”€â”€ utils.ts                          # Utility functions (format, calc, etc.)

Styles:
â””â”€â”€ globals.css                       # TailwindCSS + custom styles
```

---

### Frontend Development Phases

**Phase 1: Core Dashboard (Week 1-2)**
- [ ] Next.js project setup + TypeScript
- [ ] Layout (Header, Sidebar, Main area)
- [ ] Dashboard page (static metrics first, then real-time)
- [ ] API endpoints for dashboard data
- [ ] WebSocket setup (listen to executed_trades)
- [ ] Trades table + sorting

**Phase 2: Live Signals & Details (Week 2-3)**
- [ ] Signals stream page (real-time WebSocket)
- [ ] Expandable reasoning traces
- [ ] Trade details modal with full LLM reasoning
- [ ] Charts (P&L over time, portfolio allocation)
- [ ] Mode switcher

**Phase 3: Polish & Monitoring (Week 3-4)**
- [ ] Positions page
- [ ] Settings page (mode, config)
- [ ] Agent monitoring + logs
- [ ] Reasoning debugger page
- [ ] Dark mode toggle
- [ ] Mobile responsiveness
- [ ] Error handling + toast alerts

**Phase 4: Advanced Features (Phase 2)**
- [ ] Export trades as CSV
- [ ] Email alerts for trades
- [ ] Custom dashboard layouts
- [ ] Performance comparison (mode vs mode)
- [ ] Backtesting UI integration

---

### Frontend Data Types (Shared with Backend)

```typescript
// Dashboard Summary
interface DashboardData {
  portfolio: {
    totalBalance: number;
    todayPnL: number;
    todayPnLPercent: number;
    allTimePnL: number;
    allTimePnLPercent: number;
    allocations: { ticker: string; value: number; percent: number }[];
  };
  metrics: {
    sharpeRatio: number;
    winRate: number;
    maxDrawdown: number;
    profitFactor: number;
    totalTrades: number;
  };
  agentStatus: { agent: string; status: "running" | "stopped" | "error"; lastHeartbeat: Date }[];
  recentTrades: Trade[];
}

// Trade (full details)
interface Trade {
  orderId: string;
  ticker: string;
  action: "BUY" | "SELL";
  entryPrice: number;
  exitPrice?: number;
  size: number;
  entryTime: Date;
  exitTime?: Date;
  pnl: number;
  pnlPercent: number;
  mode: "buffett" | "soros" | "cathie" | "contrarian";
  status: "open" | "closed";
  reasoning: {
    newsSignal: string;
    investmentAnalysis: string;
    personalityAdjustment: string;
    brokerExecution: string;
  };
}

// Signal
interface Signal {
  signalId: string;
  agent: "news" | "investment" | "personality";
  ticker: string;
  type: "sentiment" | "trade_recommendation" | "validation";
  sentiment?: number; // -1 to 1
  confidence: number; // 0 to 1
  action?: "BUY" | "SELL" | "HOLD";
  timestamp: Date;
  reasoning: string;
  llmTrace: {
    prompt: string;
    response: string;
    toolsCalled: string[];
  };
}

// Position
interface Position {
  ticker: string;
  entryPrice: number;
  currentPrice: number;
  size: number;
  unrealizedPnL: number;
  unrealizedPnLPercent: number;
  entryTime: Date;
  daysHeld: number;
  mode: string;
  stopLoss?: number;
  takeProfit?: number;
}
```

---

### WebSocket Event Format

```typescript
// Frontend subscribes to WebSocket and receives:

// 1. New executed trade
{
  type: "trade_executed",
  data: { orderId, ticker, action, entryPrice, size, timestamp, mode }
}

// 2. Position P&L update (every 2-5 sec)
{
  type: "position_update",
  data: { ticker, currentPrice, unrealizedPnL, unrealizedPnLPercent, timestamp }
}

// 3. New signal processed
{
  type: "signal_received",
  data: { signalId, ticker, agent, sentiment, confidence, action, timestamp }
}

// 4. Metrics update
{
  type: "metrics_update",
  data: { todayPnL, todayPnLPercent, portfolioMetrics: {...}, timestamp }
}

// 5. Agent status change
{
  type: "agent_status",
  data: { agent, status, lastHeartbeat, error? }
}
```

---

## ğŸ§  LLM Reasoning Visibility

One key feature of this MVP: **trace LLM reasoning for every decision**. Each agent logs:
- Input data (signals, context)
- LLM system prompt used
- LLM response (reasoning, analysis)
- Final decision made
- Confidence and adjustments

This is logged to SQLite and displayed in Streamlit for debugging and validation.

---

## ğŸ“ˆ Future Phases

### Phase 2: Hybrid Reasoning & Tool Improvement
- [ ] Add more sophisticated LLM tools (e.g., sector comparison, insider buying analysis)
- [ ] Implement dynamic persona weighting (blend modes based on market regime)
- [ ] Add RL-based feedback: learn from trade outcomes, improve prompts

### Phase 3: Multi-Asset & Advanced Reasoning
- [ ] Expand to US large-cap stocks
- [ ] Add options (covered calls, protective puts)
- [ ] Multi-model reasoning (ensemble of Claude + GPT-4 for better accuracy)

### Phase 4: Production Hardening
- [ ] Distributed deployment (Kubernetes)
- [ ] Real-time monitoring & alerting
- [ ] LLM fine-tuning on historical trading data
- [ ] Advanced risk management (correlation hedging, dynamic position sizing)

### Phase 5: Autonomous Learning
- [ ] Store all trades + LLM reasoning in vector DB
- [ ] Continuous prompt optimization based on performance
- [ ] Learn when to switch personas automatically based on market regime

---

## âš–ï¸ Regulatory & Compliance Notes

**MVP (Paper Trading):**
- âœ… No registration required.
- âœ… Can run on personal account.
- âœ… Backtest & paper trade freely.
- â„¹ï¸ LLM reasoning is fully traceable and explainable (good for compliance).

**If Going Live (Real Money):**
- âš ï¸ Consult lawyer â€” may need SEC registration if managing others' money.
- âœ… LLM-based systems have **explainability advantage** â€” every trade has reasoning logged.
- âš ï¸ Maintain audit logs of all LLM reasoning.
- âš ï¸ Consider liability insurance if scaling to fund.

---

## ğŸ“ Key Design Principles

1. **LLM-First Reasoning:** Agents use LLM to reason through decisions, not hard-coded rules.
2. **Explainability:** Every trade decision includes full LLM reasoning trace.
3. **Tool-Based:** LLM agents use structured tools (sentiment, quant, portfolio checks) for accurate, verifiable outputs.
4. **Modular Agents:** Each agent is independent LangGraph state graph; can be tested/debugged in isolation.
5. **Stateless Reasoning:** LLM prompts are deterministic; state lives in Kafka, Redis, SQLite.
6. **Observable:** Full reasoning traces logged for every decision (debugging + compliance).

---

## ğŸ” Debugging LLM Reasoning

To understand how the LLM agents are reasoning:

1. **Check Reasoning Traces:** Streamlit dashboard displays full LLM reasoning for each trade
2. **SQLite Logs:** `trades.db` stores decision_reasoning column with full traces
3. **Pino Logs:** Structured JSON logs include LLM prompts, responses, and confidence
4. **Replay Mode:** Backtest can replay past signals and show what LLM would decide today vs. then

---

## ğŸ¤ Contributing & Testing

- **Unit Tests:** Test individual agents with mock LLM responses
- **Integration Tests:** Test full Kafka pipeline with real LLM (Claude/GPT)
- **Backtests:** Always backtest new agent logic before live deployment
- **Reasoning Review:** Manually review LLM reasoning traces for quality

---

## ğŸ“ Support & Questions

For architecture questions on LLM reasoning loops, agent design, or tool integration, refer to this README. Implementation details in code docstrings.

---

**Last Updated:** October 29, 2025
**Status:** MVP Architecture (LLM-Powered Agents with LangGraph) â€” Ready for Development Phase 1
**Language:** TypeScript + Node.js
**Agent Framework:** LangGraph (not rule-based)
